Meanwhile in the #hijax discord channel
> At the risk of sounding like a CA fanatic I would like to see you train a neural network to implement the update rule for Conway's Game of Life. The solution would be a combination of your challenge1 and the solution to workshop3 and, optionally for more practice with equinox, workshop4.

TODO:
1. Is it even possible? On pen and paper (or equivalent) design a small neural network architecture that implements the mapping from a 3x3 grid of life cells to the next state of the middle cell. I haven't done this but I figure it would only take a network with an input layer (9 units) and two or three hidden layers with maybe around 3 hidden units each and then one linear output unit which you can then transform into a probability of life using logistic sigmoid / softmax. You should probably use ReLU(z) = max(0,z) as the nonlinearity for the hidden neurons and some neurons will probably need biases.
2. Workshop 3: study the workshop 3 code on the solutions branch. You should hopefully be able to see how to (a) modify the network architecture to represent your architecture from step (1), and (b) see how to replace the MNIST dataset with some 3x3 life neighbourhoods as x_train and some ground truth next state labels as y_train.
3. Write the code: Merge the workshop 3 solution with your challenge 1 solution to implement the changes I described in step (2). For the neighbourhoods, you will need to generate them either randomly or by taking slices of your challenge 1 game of life simulation. Either way, you can generate the ground truth labels using your challenge 1 implementation of the rules.
4. Does it learn? Run it and see if you can get a reasonable loss/accuracy. Disclaimer: I haven't tried this before so idk if it will work!

Next steps (choose your adventure):

5. Does it learn what you expect? If it learns, do the weights and biases of the network correspond to the ones you designed in step (1)?
6. "Generative AI": Plug your learned network into a life simulation like challenge 1 and see how it evolves. If it's 100% accurate it will go fine but if there are subtle errors these could introduce interesting dynamics.
7. Convolutional neural networks: So far you have essentially implemented a '2d convolution operator' which is a small network applied to part of a grid and then you repeatedly call that operator over each 3x3 window into the whole life state to produce a new life state. This is the same approach that convolutional layers take to process images into internal representations in computer vision parts of deep learning. As a way to explore equinox, rewrite your network as a conv net that takes a whole life state as input and produces the next state as output rather than operating on single neighbourhoods. Workshop 4 code (see wip branch for draft) could help. Also equinox has a CNN tutorial that could help (and it includes written commentary unlike hijax workshop 4 draft code). It could make sense to leave this for after workshop 4.

## Additional advice

> I'm a little fuzzy about what the output of the network should be. If you want it to be "0 when the next cell should be dead and 1 when it should be alive" it's a bit tricky to turn this into a good loss function. If put the output of the last unit through a logistic sigmoid (jax.nn.sigmoid) you can interpret the output as a probability and use cross entropy from the ground truth (as a discrete dirac distribution) as a loss function (this is what workshop 3 does for MNIST with a 10-dimensional logistic sigmoid also called softmax). Then when simulating you can say 'if the output is >0 (the probability output is >0.5) call the next cell alive'. 
> So for the hand-designed network you could either stop when the output is +ve for alive or -ve for dead, or you could make it "as positive as possible" for alive and "as negative as possible" for dead.
> I think hand-designing the network should not be so hard. This solution shows that you only need to find ways to make the neurons implement a logic/arithmetic circuit with only +, ==2, &, and | gates. It might help to see if you can find a way to implement each of these independently (with a 2-input, 1-output configuration, with hidden units if necessary) then the larger network will be a composition of some of these building blocks. 
